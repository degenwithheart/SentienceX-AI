# .env.example for SentienceX-AI
# Configure Hugging Face model IDs or local paths for the three BERT roles.
# Replace these with your retrained model IDs or local directories when available.

# === MODEL CONFIGURATION ===

# BERT Base for sentiment analysis (lightweight)
SENTIMENT_MODEL=nlptown/bert-base-multilingual-uncased-sentiment

# BERT Base for threat detection (contextual threat cues)
THREAT_MODEL=mrm8488/distilbert-base-uncased-finetuned-sentiment

# BERT Large for sarcasm detection (captures tone and nuance)
SARCASM_MODEL=bert-large-uncased

# Response generation model (GPT-2 or GPT-Neo recommended)
RESPONSE_MODEL=gpt2

# Device setting: 'cpu' or 'cuda'
MODEL_DEVICE=cpu

# === LOCAL MODEL MODE ===
# Feature flag: if true, force the system to load models from local paths.
# Set to 'true' for offline/local inference, 'false' to use Hugging Face.
USE_LOCAL_MODELS=false

# Optional: custom cache directory (default: backend/app/saved_model)
# MODEL_CACHE_DIR=C:\Users\Styuart Mozli\project\ai\brain\cache

# === LOCAL MODEL EXAMPLES ===
# Uncomment and modify these if you use local retrained models:
#
# SENTIMENT_MODEL=C:\Users\Styuart Mozli\project\bert_models\bert_base
# THREAT_MODEL=C:\Users\Styuart Mozli\project\bert_models\bert_base
# SARCASM_MODEL=C:\Users\Styuart Mozli\project\bert_models\bert_large
# RESPONSE_MODEL=C:\Users\Styuart Mozli\project\gpt_models\gpt_neo

# === OPTIONAL INFRASTRUCTURE SETTINGS ===
# RABBITMQ_URL=amqp://user:pass@localhost:5672/
# REDIS_URL=redis://localhost:6379/0
# If you use TLS for Redis (e.g., managed Redis with TLS) provide a rediss URL:
# REDIS_URL=rediss://:password@redis-host:6380/0
# DATABASE_URL=sqlite:///./backend/app/dataset/logs.db

# === LOCAL TLS / PROXY (Caddy) NOTES ===
# The docker-compose file includes an optional Caddy service that provides
# an internal CA and TLS termination for local development. It reverse-proxies
# https://localhost to the backend container on port 8000. To use it:
# - Ensure you have Docker installed
# - Build the images (see README) then run `docker-compose up caddy` to start
# - Follow the README instructions to trust the generated internal CA in your OS/browser
